---
title: "Best Practice Research"
author: "Jenna Reps, Peter R. Rijnbeek"
date: '`r Sys.Date()`'
header-includes:
    - \usepackage{fancyhdr}
    - \pagestyle{fancy}
    - \fancyhead{}
    - \fancyhead[CO,CE]{Installation Guide}
    - \fancyfoot[CO,CE]{PatientLevelPrediction Package Version `r    utils::packageVersion("PatientLevelPrediction")`}
    - \fancyfoot[LE,RO]{\thepage}
    - \renewcommand{\headrulewidth}{0.4pt}
    - \renewcommand{\footrulewidth}{0.4pt}
output:
  pdf_document:
    includes:
      in_header: preamble.tex
    number_sections: yes
    toc: yes
  word_document:
    toc: yes
  html_document:
    number_sections: yes
    toc: yes
---
<!--
%\VignetteEngine{knitr}
%\VignetteIndexEntry{Best Practices}
-->

## Best practice publications using the OHDSI PatientLevelPrediction framework

<table>
<tr>
<th>
Topic
</th>
<th>
Research Summary
</th>
<th>
Link
</th>
</tr>


<tr>
<td>
Problem Specification
</td>
<td>
When is prediction suitable in observational data?
</td>
<td>
Guidelines needed
</td>
</tr>


<tr>
<td>
Data Creation
</td>
<td>
Comparison of cohort vs case-control design
</td>
<td>
<a href='https://journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00501-2'>Journal of Big Data</a>
</td>
</tr>

<tr>
<td>
Data Creation
</td>
<td>
Addressing loss to follow-up (right censoring)	
</td>
<td>
<a href='https://link.springer.com/article/10.1186/s12911-021-01408-x'>BMC medical informatics and decision makingk</a>
</td>
</tr>

<tr>
<td>
Data Creation
</td>
<td>
Investigating how to address left censoring in features construction
</td>
<td>
<a href='https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-021-01370-2'>BMC Medical Research Methodology</a>
</td>
</tr>

<tr>
<td>
Data Creation	
</td>
<td>
Impact of over/under-sampling
</td>
<td>
Study being developed
</td>
</tr>

<tr>
<td>
Data Creation	
</td>
<td>
Impact of phenotypes
</td>
<td>
Study Done - Paper submitted
</td>
</tr>

<tr>
<td>
Model development	
</td>
<td>
How much data do we need for prediction - Learning curves at scale
</td>
<td>
<a href='https://arxiv.org/abs/2008.07361'>Preprint link</a>
</td>
</tr>

<tr>
<td>
Model development	
</td>
<td>
What impact does test/train/validation design have on model performance
</td>
<td>
<a href='https://bmjopen.bmj.com/content/11/12/e050146'>BMJ Open </a>
</td>
</tr>

<tr>
<td>
Model development	
</td>
<td>
What is the impact of the classifier 
</td>
<td>
<a href='https://academic.oup.com/jamia/article/25/8/969/4989437?login=true'>JAMIA</a>
</td>
</tr>

<tr>
<td>
Model development	
</td>
<td>
Can we find hyper-parameter combinations per classifier that consistently lead to good performing models when using claims/EHR data?	
</td>
<td>
Study needs to be done
</td>
</tr>

<tr>
<td>
Model development	
</td>
<td>
Can we use ensembles to combine models developed using different databases to improve models transportability?
</td>
<td>
<a href=''> Paper under review at BMC </a>
</td>
</tr>

<tr>
<td>
Evaluation
</td>
<td>
How should we present model performance? (e.g., new visualizations)
</td>
<td>
<a href='https://academic.oup.com/jamiaopen/article/4/1/ooab017/6168493?searchresult=1'>JAMIA Open</a>
</td>
</tr>

<tr>
<td>
Evaluation
</td>
<td>
How to interpret external validation performance (can we figure out why the performance drops or stays consistent)?	
</td>
<td>
Study needs to be done
</td>
</tr>

<tr>
<td>
Evaluation
</td>
<td>
Recalibration methods
</td>
<td>
Study needs to be done
</td>
</tr>

<tr>
<td>
Evaluation
</td>
<td>
Is there a way to automatically simplify models?	
</td>
<td>
<a href='https://ohdsi-studies.github.io/FeatureSelectionComparison/docs/Protocol.html'>Study protocol under development </a>
</td>
</tr>
</table>
    